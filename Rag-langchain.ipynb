{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da5df51f-b5ca-4104-8b75-a65134093539",
   "metadata": {},
   "source": [
    "#### Importing Necessary Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b4ba2c4-5edf-4c94-a2aa-5067ae9e96d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS \n",
    "import openai\n",
    "import import_ipynb\n",
    "from Config import *\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f2ba77-5e0a-4707-8ad1-f88c789d79b9",
   "metadata": {},
   "source": [
    "##### Environmental Variables (api_key for OPENAI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0cabd15-369a-4f7b-9ee1-c78eb25e0f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_TYPE\"] = key_value_dict['api_type']\n",
    "os.environ[\"OPENAI_API_VERSION\"] = key_value_dict['api_version']\n",
    "os.environ[\"OPENAI_API_BASE\"] = key_value_dict['api_base']\n",
    "os.environ[\"OPENAI_API_KEY\"] = key_value_dict['api_key']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66f8d833-38bd-401b-bd4b-de1195f36a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# location of the pdf file/files. \n",
    "doc_reader = PdfReader('knn.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b349cb24-1966-459e-bb98-643c834255e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data from the file and put them into a variable called raw_text\n",
    "raw_text = ''\n",
    "for i, page in enumerate(doc_reader.pages):\n",
    "    text = page.extract_text()\n",
    "    if text:\n",
    "        raw_text += text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5540e123-5c53-4cb9-9d43-6f1060037b64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6449"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f23bd2c-3120-4cf1-bede-04554e3d77d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'K-Nearest Neighbour\\nTushar B. Kute,\\nhttp://tusharkute.com\\nWhat sort of Machine Learning?\\n•An idea that can be used for machine learning—\\nas does another maxim involving poultry: \"birds \\nof a feather flock together.\" \\n•In other words, things that are alike are likely to \\nhave properties that are alike. \\n•We can use this principle to classify data by \\nplacing it in the category with the most similar, \\nor \"nearest\" neighbors.\\nNearest Neighbor Classification\\n•In a single sentence, nearest neighbor classifiers are defined \\nby their characteristic of classifying unlabeled examples by \\nassigning them the class of the most similar labeled examples. \\nDespite the simplicity of this idea, nearest neighbor methods \\nare extremely powerful. They have been used successfully for:\\n–Computer vision applications, including optical character \\nrecognition and facial recognition in both still images and \\nvideo\\n–Predicting whether a person enjoys a movie which he/she \\nhas been recommended (as in the Netflix '"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_text[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bfb76a6-de7e-4758-b157-9f05884db0d8",
   "metadata": {},
   "source": [
    "## 1.Text Splitter\n",
    "### This takes the text and splits it into chunks. The chunk size is characters not tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6169de93-107f-4648-9178-e92a0ee6e7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Splitting up the text into smaller chunks for indexing\n",
    "text_splitter = CharacterTextSplitter(        \n",
    "    separator = \"\\n\",\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap  = 200 #striding over the text\n",
    ")\n",
    "texts = text_splitter.split_text(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38b9af78-bc27-4cfe-8c86-10fcbb3b9e58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['K-Nearest Neighbour\\nTushar B. Kute,\\nhttp://tusharkute.com\\nWhat sort of Machine Learning?\\n•An idea that can be used for machine learning—\\nas does another maxim involving poultry: \"birds \\nof a feather flock together.\" \\n•In other words, things that are alike are likely to \\nhave properties that are alike. \\n•We can use this principle to classify data by \\nplacing it in the category with the most similar, \\nor \"nearest\" neighbors.\\nNearest Neighbor Classification\\n•In a single sentence, nearest neighbor classifiers are defined \\nby their characteristic of classifying unlabeled examples by \\nassigning them the class of the most similar labeled examples. \\nDespite the simplicity of this idea, nearest neighbor methods \\nare extremely powerful. They have been used successfully for:\\n–Computer vision applications, including optical character \\nrecognition and facial recognition in both still images and \\nvideo\\n–Predicting whether a person enjoys a movie which he/she',\n",
       " '–Computer vision applications, including optical character \\nrecognition and facial recognition in both still images and \\nvideo\\n–Predicting whether a person enjoys a movie which he/she \\nhas been recommended (as in the Netflix challenge)\\n–Identifying patterns in genetic data, for use in detecting \\nspecific protein or diseases\\nThe kNN Algorithm\\n•The kNN algorithm begins with a training dataset \\nmade up of examples that are classified into several \\ncategories, as labeled by a nominal variable. \\n•Assume that we have a test dataset containing \\nunlabeled examples that otherwise have the same \\nfeatures as the training data. \\n•For each record in the test dataset, kNN identifies k \\nrecords in the training data that are the \"nearest\" in \\nsimilarity, where k is an integer specified in advance. \\n•The unlabeled test instance is assigned the class of \\nthe majority of the k nearest neighbors\\nExample:\\nReference: Machine Learning with R, Brett Lantz, Packt PublishingExample:\\nExample:\\nClassify me now!',\n",
       " \"the majority of the k nearest neighbors\\nExample:\\nReference: Machine Learning with R, Brett Lantz, Packt PublishingExample:\\nExample:\\nClassify me now!\\nExample:\\nCalculating Distance\\n•Locating the tomato's nearest neighbors requires \\na distance function, or a formula that measures \\nthe similarity between two instances.\\n•There are many different ways to calculate \\ndistance. \\n•Traditionally, the kNN algorithm uses Euclidean \\ndistance, which is the distance one would measure \\nif you could use a ruler to connect two points, \\nillustrated in the previous figure by the dotted \\nlines connecting the tomato to its neighbors.\\nCalculating Distance\\nReference: Super Data ScienceDistance\\n•Euclidean distance is specified by the following formula, where p \\nand q are th examples to be compared, each having n features. The \\nterm p1 refers to the value of the first feature of example p, while \\nq1 refers to the value of the first feature of example q:\",\n",
       " 'and q are th examples to be compared, each having n features. The \\nterm p1 refers to the value of the first feature of example p, while \\nq1 refers to the value of the first feature of example q:\\n•The distance formula involves comparing the values of each \\nfeature. For example, to calculate the distance between the \\ntomato (sweetness = 6, crunchiness = 4), and the green bean \\n(sweetness = 3, crunchiness = 7), we can use the formula as follows:\\nDistance\\nDistance\\nManhattan Distance\\nEuclidean DistanceClosest Neighbors\\nChoosing appropriate k\\n•Deciding how many neighbors to use for kNN \\ndetermines how well the mode will generalize to \\nfuture data. \\n•The balance between overfitting and underfitting \\nthe training data is a problem known as the bias-\\nvariance tradeoff. \\n•Choosing a large k reduces the impact or variance \\ncaused by noisy data, but can bias the learner such \\nthat it runs the risk of ignoring small, but important \\npatterns.\\nChoosing appropriate k\\nChoosing appropriate k',\n",
       " 'caused by noisy data, but can bias the learner such \\nthat it runs the risk of ignoring small, but important \\npatterns.\\nChoosing appropriate k\\nChoosing appropriate k\\n•In practice, choosing k depends on the difficulty \\nof the concept to be learned and the number of \\nrecords in the training data. \\n•Typically, k is set somewhere between 3 and 10. \\nOne common practice is to set k equal to the \\nsquare root of the number of training examples. \\n•In the classifier, we might set k = 4, because \\nthere were 15 example ingredients in the \\ntraining data and the square root of 15 is 3.87.\\nMin-Max normalization\\n•The traditional method of rescaling features for kNN is min-\\nmax normalization.\\n•This process transforms a feature such that all of its values fall \\nin a range between 0 and 1. The formula for normalizing a \\nfeature is as follows. Essentially, the formula subtracts the \\nminimum of feature X from each value and divides by the range \\nof X:',\n",
       " 'in a range between 0 and 1. The formula for normalizing a \\nfeature is as follows. Essentially, the formula subtracts the \\nminimum of feature X from each value and divides by the range \\nof X:\\n•Normalized feature values can be interpreted as indicating how \\nfar, from 0 percent to 100 percent, the original value fell along \\nthe range between the original minimum and maximum.\\nThe Lazy Learning\\n•Using the strict definition of learning, a lazy learner \\nis not really learning anything.\\n•Instead, it merely stores the training data in it. This \\nallows the training phase to occur very rapidly, with \\na potential downside being that the process of \\nmaking predictions tends to be relatively slow. \\n•Due to the heavy reliance on the training instances, \\nlazy learning is also known as instance-based \\nlearning  or rote learning.\\nFew Lazy Learning Algorithms\\n•K Nearest Neighbors\\n•Local Regression\\n•Lazy Naive Bayes\\nThe kNN Algorithm\\nPython Packages needed\\n•pandas\\n–Data Analytics\\n•numpy',\n",
       " 'learning  or rote learning.\\nFew Lazy Learning Algorithms\\n•K Nearest Neighbors\\n•Local Regression\\n•Lazy Naive Bayes\\nThe kNN Algorithm\\nPython Packages needed\\n•pandas\\n–Data Analytics\\n•numpy\\n–Numerical Computing\\n•matplotlib.pyplot\\n–Plotting graphs\\n•sklearn\\n–Classification and Regression Classes\\nSample Application\\nKNN – Classification : Dataset\\n•The best small project to start with on a new tool is the \\nclassification of iris flowers (e.g. the iris dataset).\\n•This is a good project because it is so well understood.\\n–Attributes are numeric so you have to figure out how to load and \\nhandle data.\\n–It is a classification problem, allowing you to practice with perhaps \\nan easier type of supervised learning algorithm.\\n–It is a multi-class classification problem (multi-nominal) that may \\nrequire some specialized handling.\\n–It only has 4 attributes and 150 rows, meaning it is small and easily \\nfits into memory (and a screen or A4 page).',\n",
       " 'require some specialized handling.\\n–It only has 4 attributes and 150 rows, meaning it is small and easily \\nfits into memory (and a screen or A4 page).\\n–All of the numeric attributes are in the same units and the same \\nscale, not requiring any special scaling or transforms to get started.\\nDataset \\nKNN – Classification : Dataset\\nPre-processing\\nCharacterize\\nFinding error with changed k\\nError rate\\nUseful resources\\n•www.mitu.co.in  \\n•www.pythonprogramminglanguage.com\\n•www.scikit-learn.org   \\n•www.towardsdatascience.com\\n•www.medium.com\\n•www.analyticsvidhya.com\\n•www.kaggle.com\\n•www.stephacking.com\\n•www.github.com  tushar@tusharkute.com      Thank you\\nThis presentation is created using LibreOffice Impress 5.1.6.2, can be used freely as per GNU General Public License\\nWeb Resources\\nhttps://mitu.co.in  \\nhttp://tusharkute.com\\n/mITuSkillologies\\n @mitu_group\\ncontact@mitu.co.in\\n/company/mitu-\\nskillologies\\nMITUSkillologies']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd33d35a-79b4-4bea-86aa-bdcdb2a9e1b2",
   "metadata": {},
   "source": [
    "## 2.Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ecb4e527-bbe8-40c7-af22-5de6d71818da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import AzureOpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b171cef2-5e58-4345-8167-1b34ee542b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    deployment=key_value_dict[\"embed_eng_dep_nm\"],\n",
    "    model=key_value_dict[\"embedding_model\"],\n",
    "    chunk_size=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "91c93766-9821-4e9f-acdc-7d1a990254c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "docsearch = FAISS.from_texts(texts, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "101e5ff7-005b-4ef9-aab3-32c14f7d7c57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x1eefb6c2650>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bb0694c4-182a-49e1-8ba6-87df928a535a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#docsearch.embedding_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "76866fdf-125e-402b-bd9e-d0a6aa42ec27",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Where does knn is significantly used?\"\n",
    "docs = docsearch.similarity_search(query,k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "903ae4df-0c90-4bb4-853d-932264e00b77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='–Computer vision applications, including optical character \\nrecognition and facial recognition in both still images and \\nvideo\\n–Predicting whether a person enjoys a movie which he/she \\nhas been recommended (as in the Netflix challenge)\\n–Identifying patterns in genetic data, for use in detecting \\nspecific protein or diseases\\nThe kNN Algorithm\\n•The kNN algorithm begins with a training dataset \\nmade up of examples that are classified into several \\ncategories, as labeled by a nominal variable. \\n•Assume that we have a test dataset containing \\nunlabeled examples that otherwise have the same \\nfeatures as the training data. \\n•For each record in the test dataset, kNN identifies k \\nrecords in the training data that are the \"nearest\" in \\nsimilarity, where k is an integer specified in advance. \\n•The unlabeled test instance is assigned the class of \\nthe majority of the k nearest neighbors\\nExample:\\nReference: Machine Learning with R, Brett Lantz, Packt PublishingExample:\\nExample:\\nClassify me now!'),\n",
       " Document(page_content='learning  or rote learning.\\nFew Lazy Learning Algorithms\\n•K Nearest Neighbors\\n•Local Regression\\n•Lazy Naive Bayes\\nThe kNN Algorithm\\nPython Packages needed\\n•pandas\\n–Data Analytics\\n•numpy\\n–Numerical Computing\\n•matplotlib.pyplot\\n–Plotting graphs\\n•sklearn\\n–Classification and Regression Classes\\nSample Application\\nKNN – Classification : Dataset\\n•The best small project to start with on a new tool is the \\nclassification of iris flowers (e.g. the iris dataset).\\n•This is a good project because it is so well understood.\\n–Attributes are numeric so you have to figure out how to load and \\nhandle data.\\n–It is a classification problem, allowing you to practice with perhaps \\nan easier type of supervised learning algorithm.\\n–It is a multi-class classification problem (multi-nominal) that may \\nrequire some specialized handling.\\n–It only has 4 attributes and 150 rows, meaning it is small and easily \\nfits into memory (and a screen or A4 page).')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26b0c11-6632-44f2-b739-acde5741a6a9",
   "metadata": {},
   "source": [
    "## 3.Retrival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0b374759-52e6-4f53-9a2d-b7bee503eb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.chat_models import AzureChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "45c6b881-3dd6-4739-9a9b-85e374704093",
   "metadata": {},
   "outputs": [],
   "source": [
    " llm = AzureChatOpenAI(\n",
    "            deployment_name=key_value_dict[\"comp_eng_dep_nm\"],\n",
    "            temperature=0,\n",
    "            openai_api_version=key_value_dict[\"api_version\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "630a17db-8135-4400-a569-473003504f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = load_qa_chain(llm = llm, \n",
    "                      chain_type=\"stuff\") # we are going to stuff all the docs in at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "821539de-cd0a-4386-bcdc-a86ac8b88978",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The author of the book is not mentioned in the given context.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"who is author of the book?\"\n",
    "docs = docsearch.similarity_search(query)\n",
    "chain.run(input_documents=docs, question=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8fe85fbe-e335-4ca6-8370-66576d8e7885",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain2 = load_qa_chain(llm, \n",
    "                      chain_type=\"map_rerank\",\n",
    "                      return_intermediate_steps=True\n",
    "                      ) \n",
    "\n",
    "query = \"What are the Lazy Algorithms quotted in the book?\"\n",
    "docs = docsearch.similarity_search(query,k=2)\n",
    "results = chain2({\"input_documents\": docs, \"question\": query}, return_only_outputs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "041c6ca3-78cb-4866-9486-2801f1160c93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'intermediate_steps': [{'answer': 'K Nearest Neighbors, Local Regression, Lazy Naive Bayes',\n",
       "   'score': '100'},\n",
       "  {'answer': 'K Nearest Neighbors, Local Regression, Lazy Naive Bayes',\n",
       "   'score': '100'}],\n",
       " 'output_text': 'K Nearest Neighbors, Local Regression, Lazy Naive Bayes'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "135e55ab-0441-44b4-a526-bed127e29668",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'K Nearest Neighbors, Local Regression, Lazy Naive Bayes'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['output_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c923bd-f51b-4da2-aa09-1c368dcbe124",
   "metadata": {},
   "source": [
    "## Retrival QAChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a1af8484-936b-4f9d-a5be-656f3104db87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# set up FAISS as a generic retriever \n",
    "retriever = docsearch.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":4})\n",
    "\n",
    "# create the chain to answer questions \n",
    "rqa = RetrievalQA.from_chain_type(llm, \n",
    "                                  chain_type=\"stuff\", \n",
    "                                  retriever=retriever, \n",
    "                                  return_source_documents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a46e84b6-d1b6-4d08-ab9d-24ddffa7a66b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'what is knn?',\n",
       " 'result': 'KNN stands for K-Nearest Neighbors, which is a machine learning algorithm used for classification and regression. It is based on the principle that similar things are likely to have similar properties. In KNN, a test instance is classified by finding the K training instances that are closest to it in terms of distance and assigning the test instance the class that is most common among its K nearest neighbors.',\n",
       " 'source_documents': [Document(page_content='–Computer vision applications, including optical character \\nrecognition and facial recognition in both still images and \\nvideo\\n–Predicting whether a person enjoys a movie which he/she \\nhas been recommended (as in the Netflix challenge)\\n–Identifying patterns in genetic data, for use in detecting \\nspecific protein or diseases\\nThe kNN Algorithm\\n•The kNN algorithm begins with a training dataset \\nmade up of examples that are classified into several \\ncategories, as labeled by a nominal variable. \\n•Assume that we have a test dataset containing \\nunlabeled examples that otherwise have the same \\nfeatures as the training data. \\n•For each record in the test dataset, kNN identifies k \\nrecords in the training data that are the \"nearest\" in \\nsimilarity, where k is an integer specified in advance. \\n•The unlabeled test instance is assigned the class of \\nthe majority of the k nearest neighbors\\nExample:\\nReference: Machine Learning with R, Brett Lantz, Packt PublishingExample:\\nExample:\\nClassify me now!'),\n",
       "  Document(page_content=\"the majority of the k nearest neighbors\\nExample:\\nReference: Machine Learning with R, Brett Lantz, Packt PublishingExample:\\nExample:\\nClassify me now!\\nExample:\\nCalculating Distance\\n•Locating the tomato's nearest neighbors requires \\na distance function, or a formula that measures \\nthe similarity between two instances.\\n•There are many different ways to calculate \\ndistance. \\n•Traditionally, the kNN algorithm uses Euclidean \\ndistance, which is the distance one would measure \\nif you could use a ruler to connect two points, \\nillustrated in the previous figure by the dotted \\nlines connecting the tomato to its neighbors.\\nCalculating Distance\\nReference: Super Data ScienceDistance\\n•Euclidean distance is specified by the following formula, where p \\nand q are th examples to be compared, each having n features. The \\nterm p1 refers to the value of the first feature of example p, while \\nq1 refers to the value of the first feature of example q:\"),\n",
       "  Document(page_content='K-Nearest Neighbour\\nTushar B. Kute,\\nhttp://tusharkute.com\\nWhat sort of Machine Learning?\\n•An idea that can be used for machine learning—\\nas does another maxim involving poultry: \"birds \\nof a feather flock together.\" \\n•In other words, things that are alike are likely to \\nhave properties that are alike. \\n•We can use this principle to classify data by \\nplacing it in the category with the most similar, \\nor \"nearest\" neighbors.\\nNearest Neighbor Classification\\n•In a single sentence, nearest neighbor classifiers are defined \\nby their characteristic of classifying unlabeled examples by \\nassigning them the class of the most similar labeled examples. \\nDespite the simplicity of this idea, nearest neighbor methods \\nare extremely powerful. They have been used successfully for:\\n–Computer vision applications, including optical character \\nrecognition and facial recognition in both still images and \\nvideo\\n–Predicting whether a person enjoys a movie which he/she'),\n",
       "  Document(page_content='caused by noisy data, but can bias the learner such \\nthat it runs the risk of ignoring small, but important \\npatterns.\\nChoosing appropriate k\\nChoosing appropriate k\\n•In practice, choosing k depends on the difficulty \\nof the concept to be learned and the number of \\nrecords in the training data. \\n•Typically, k is set somewhere between 3 and 10. \\nOne common practice is to set k equal to the \\nsquare root of the number of training examples. \\n•In the classifier, we might set k = 4, because \\nthere were 15 example ingredients in the \\ntraining data and the square root of 15 is 3.87.\\nMin-Max normalization\\n•The traditional method of rescaling features for kNN is min-\\nmax normalization.\\n•This process transforms a feature such that all of its values fall \\nin a range between 0 and 1. The formula for normalizing a \\nfeature is as follows. Essentially, the formula subtracts the \\nminimum of feature X from each value and divides by the range \\nof X:')]}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rqa(\"what is knn?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "933bf6d7-fc08-4d23-90f7-ac5d0f400a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The kNN (k-Nearest Neighbors) algorithm works by first training a dataset with labeled examples. Then, for each unlabeled example in the test dataset, the algorithm identifies the k nearest labeled examples in the training dataset based on similarity. The unlabeled example is then assigned the class of the majority of the k nearest neighbors. The value of k is specified in advance and typically set between 3 and 10, depending on the difficulty of the concept to be learned and the number of records in the training data. The algorithm can be used for various applications, including computer vision, genetic data analysis, and predicting user preferences.\n"
     ]
    }
   ],
   "source": [
    "query = \"how does the knn classifier works?\"\n",
    "print(rqa(query)['result'],sep = \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b0875dbd-2dae-41c5-88a6-b8e93549f8f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To calculate distance in KNN, a distance function or formula is used to measure the similarity between two instances. The most commonly used distance function in KNN is Euclidean distance, which is the distance one would measure if you could use a ruler to connect two points. The formula for Euclidean distance is:\n",
      "\n",
      "d(p, q) = sqrt((p1 - q1)^2 + (p2 - q2)^2 + ... + (pn - qn)^2)\n",
      "\n",
      "where p and q are the examples to be compared, each having n features. The term p1 refers to the value of the first feature of example p, while q1 refers to the value of the first feature of example q. Other distance functions that can be used in KNN include Manhattan distance and Minkowski distance.\n"
     ]
    }
   ],
   "source": [
    "query = \"how to calculate distance in KNN?\"\n",
    "print(rqa(query)['result'],sep = \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ff326a29-3ebb-4b30-82aa-8bdc2ada8ccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choosing k in KNN depends on the difficulty of the concept to be learned and the number of records in the training data. Typically, k is set somewhere between 3 and 10. One common practice is to set k equal to the square root of the number of training examples. Choosing a large k reduces the impact or variance caused by noisy data, but can bias the learner such that it runs the risk of ignoring small, but important patterns. Therefore, deciding how many neighbors to use for KNN determines how well the model will generalize to future data. The balance between overfitting and underfitting the training data is a problem known as the bias-variance tradeoff.\n"
     ]
    }
   ],
   "source": [
    "query = \"how to choose k in KNN?\"\n",
    "print(rqa(query)['result'],sep = \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bed98be1-a91a-44ed-a241-1b2e2fabf3e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Python packages needed to implement KNN are pandas for data analytics, numpy for numerical computing, matplotlib.pyplot for plotting graphs, and sklearn for classification and regression classes.\n"
     ]
    }
   ],
   "source": [
    "query = \"What are the packages need to implement KNN?\"\n",
    "print(rqa(query)['result'],sep = \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c18fc8-9b0d-4e6f-9821-a847dd3ba965",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
